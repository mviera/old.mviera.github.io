<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Crononauta on Manuel Viera </title>
      <generator uri="https://hugo.spf13.com">Hugo</generator>
    <link>http://mviera.io/tags/crononauta/index.xml/</link>
    <language>es-es</language>
    
    
    <updated>Mon, 08 Feb 2016 00:30:53 CET</updated>
    
    <item>
      <title>Compartiendo ficheros con GlusterFS</title>
      <link>http://mviera.io/blog/compartiendo-ficheros-con-glusterfs/</link>
      <pubDate>Mon, 08 Feb 2016 00:30:53 CET</pubDate>
      
      <guid>http://mviera.io/blog/compartiendo-ficheros-con-glusterfs/</guid>
      <description>

&lt;p&gt;Cuando aparece la necesidad de escalar una aplicación web, o cualquier otro servicio, en
Internet; también aparecen nuevos problemas a resolver que antes incluso ni habíamos
reparado en ellos. Uno de estos problemas es &lt;strong&gt;la compartición de ficheros&lt;/strong&gt; entre
servidores.&lt;/p&gt;

&lt;p&gt;En mi opinión, a día de hoy éste sigue siendo un dolor de cabeza para muchos Sysadmins
o Systems Architects. Hay muchas formas de compartir ficheros entre servidores como NFS,
S3FS, FileConveyor, un crontab con un rsync entre servidores. Hay muchas soluciones,
algunas mejores y otras peores; pero hoy quería compartir la que hasta ahora creo que
puede ser una buena solución para compartir ficheros, usando &lt;strong&gt;GlusterFS&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.glusterfs.org&#34;&gt;GlusterFS&lt;/a&gt; es un sistema de ficheros escalable en red, con el
que poder crear soluciones de almacenamiento distribuidos por la red y de gran capacidad.
Ni que decir tiene que &lt;strong&gt;GlusterFS&lt;/strong&gt; es software libre.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Antes de empezar&lt;/h2&gt;

&lt;p&gt;Normalmente en los despliegues que hacemos en AWS en &lt;a href=&#34;http://crononauta.com&#34;&gt;Crononauta&lt;/a&gt;,
siempre utilizamos un mínimo de un par de instancias EC2 para el sistema de almacenamiento
en red con &lt;strong&gt;GlusterFS&lt;/strong&gt;. Además, a cada una de estas dos instancias EC2, hacemos &lt;em&gt;attach&lt;/em&gt;
de un disco EBS donde almacenaremos los ficheros en el clúster de ficheros en red. Esto
nos permite tener &lt;strong&gt;redundancia&lt;/strong&gt; en los datos, &lt;strong&gt;alta disponibilidad&lt;/strong&gt; y &lt;strong&gt;escalabilidad
horizontal&lt;/strong&gt; en el servicio. Con lo cual, tendremos un par de instancias que mantienen en
clúster de almacenamiento actuando como GlusterFS Server y por otro lado, tendremos los
clientes de GlusterFS que montarán este volumen compartido por la red.&lt;/p&gt;

&lt;p&gt;Como detalle, en &lt;strong&gt;Crononauta&lt;/strong&gt; siempre usamos Debian como distribución, en este caso
contamos con una Debian 8 (Jessie), aunque es totalmente aplicable a cualquier
distribución basada en Debian como Ubuntu y/o derivadas.&lt;/p&gt;

&lt;p&gt;Una vez tenemos esto, podemos empezar a trabajar&amp;hellip; ;-)&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Particionando los discos&lt;/h2&gt;

&lt;p&gt;Utilizaremos &lt;strong&gt;XFS&lt;/strong&gt; como sistema de ficheros para el volumen compartido en red. Por lo
que necesitaremos instalar las utilidades para trabajar con XFS, si no las tenemos ya
instaladas.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get install xfsprogs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;El siguiente paso será crear una partición en el volumen, en mi caso es &lt;code&gt;/dev/xvdf&lt;/code&gt;.
Usaremos &lt;code&gt;fdisk&lt;/code&gt; para ello:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fdisk /dev/xvdf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;y una vez en la shell interactiva de &lt;code&gt;fdisk&lt;/code&gt;, teclearemos:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;n&lt;/code&gt; para crear una nueva partición. No necesitamos especificar nada más, así que los
siguientes datos que pide podemos usar los valores por defecto que ofrece &lt;code&gt;fdisk&lt;/code&gt;, asi
que pulsaremos &lt;code&gt;Intro&lt;/code&gt; hasta finalizar. Una vez hecho esto, seguiremos dentro de la
shell interactiva de &lt;code&gt;fdisk&lt;/code&gt;, y habremos creado una partición primaria que ocupa todo
el espacio del volumen.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;w&lt;/code&gt; indicando que queremos escribir esta definición de partición en el disco y hacerla
efectiva.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Ya tenemos la partición en el volumen, así que ahora la formatearemos y la añadiremos
a nuestro &lt;code&gt;/etc/fstab&lt;/code&gt; para montarla en cada arranque del sistema. Seguiremos los
siguientes pasos:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkfs.xfs -i size=512 /dev/xvdf1
mkdir -p /export/brick1
echo &amp;quot;/dev/xvdf1 /export/brick1 xfs defaults 1 2&amp;quot;  &amp;gt;&amp;gt; /etc/fstab
mount -a &amp;amp;&amp;amp; mount
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Instalando GlusterFS&lt;/h2&gt;

&lt;p&gt;Ya tenemos el volumen preparado para GlusterFS, ahora instalaremos el servicio.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get update
apt-get install glusterfs-server glusterfs-client glusterfs-common
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Importante realizar este paso en aquellas instancias / servidores que vayan a actuar como
un GlusterFS Server.&lt;/p&gt;

&lt;p&gt;Lo siguiente que haremos será configurar nuestro fichero &lt;code&gt;/etc/hosts&lt;/code&gt; para definir unos
nombres DNS para nuestros GlusterFS Servers. También es posible hacerlo con un DNS
interno. Si vuestros servidores, como en el caso de AWS EC2, contáis con IPs privadas,
mejor usar esas. Si no, podéis usar las IPs pública.&lt;/p&gt;

&lt;p&gt;Definiremos algo como lo siguiente en &lt;code&gt;/etc/hosts&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;xxx.xxx.xxx.xxx    gfs01.example.com
xxx.xxx.xxx.xxx    gfs02.example.com
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Un anillo para gobernarlos a todos&lt;/h2&gt;

&lt;p&gt;Ya tenemos GlusterFS preparado para ser configurado. Ahora tendremos que configurar el
&lt;em&gt;anillo de confianza&lt;/em&gt; entre ambos servidores GlusterFS Server. Para ello serán necesarios
los siguientes pasos:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Desde &lt;code&gt;gfs01.example.com&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gluster peer probe gfs02.example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Desde &lt;code&gt;gfs02.example.com&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gluster peer probe gfs01.example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Es &lt;strong&gt;muy importante&lt;/strong&gt; que haya conectividad entre ambos servidores, en caso contrario, los
comandos anteriores fallarán. Si estás en AWS, asegúrate de tener bien configurado el
&lt;strong&gt;Security Group&lt;/strong&gt; para ambas instancias de storage.&lt;/p&gt;

&lt;p&gt;Si todo ha ido bien, tendremos el pool de confianza funcionando, así que lo siguiente será
definir el volumen compartido. Usaremos la siguiente instrucción desde uno de los
servidores GlusterFS:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gluster volume create gv0 replica 2 gfs01.example.com:/export/brick1/&amp;lt;volume-name&amp;gt; gfs02.example.com:/export/brick1/&amp;lt;volumen-name»
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Por último, iniciaremos el volumen que justo acabamos de crear:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gluster volume start gv0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Como detalle, podemos ver información sobre el volumen con la siguiente instrucción:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gluster volume info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Llegados a este punto, el cluster de almacenamiento debe estar totalmente operativo. Si
queremos hacer una prueba, es posible crear el punto de montaje hacia el volumen
compartido y probar si se replican correctamente los ficheros. Crearemos el punto de
montaje en &lt;code&gt;gfs01.example.com&lt;/code&gt; de la siguiente forma:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mount -t glusterfs localhost:/gv0 /mnt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ahora crearemos ficheros para comprobar si el funcionamiento de GlusterFS es correcto. Con
la siguiente instrucción crearemos diez ficheros vacíos en el punto de montaje previamente
creado:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch /mnt/test-file{1..10}.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Si GlusterFS funciona correctamente, debemos poder ver estos mismos ficheros en
&lt;code&gt;gfs02.example.com&lt;/code&gt; en el directorio &lt;code&gt;/export/brick1/&amp;lt;volume-name&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Configurando los clientes&lt;/h2&gt;

&lt;p&gt;Nos falta muy poco para terminar, ya sólo nos falta configurar los clientes de GlusterFS.
Necesitaremos tener instalado el cliente de GlusterFS &lt;code&gt;glusterfs-client&lt;/code&gt; si no lo tenemos
instalado.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-get install glusterfs-client
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hecho esto, podremos probar a montar el volumen compartido por la red, y si todo va bien
debemos ver los ficheros de prueba creados previamente.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mount -t glusterfs gfs01.example.com:/gv0 /mnt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Por último, necesitaremos un script &lt;code&gt;/etc/init.d/glusterfs-mount&lt;/code&gt; que se encargue de montar el volumen
compartido. En mi caso suelo utilizar el siguiente script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#! /bin/bash
### BEGIN INIT INFO
# Provides:          glusterfs-mount
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5 
# Default-Stop:      0 1 6 
# Short-Description: Start daemon at boot time
# Description:       Enable service provided by daemon.
### END INIT INFO
#

MOUNTPOINT=&amp;quot;/var/www/shared&amp;quot;
GLUSTERFS_SERVER=&amp;quot;gfs01.example.com&amp;quot;
GLUSTERFS_VOLUME=&amp;quot;gv0&amp;quot;

# Some things that run always at boot
mount -t glusterfs $GLUSTERFS_SERVER:/$GLUSTERFS_VOLUME $MOUNTPOINT
#
# Uncomment this line if you need to start Apache after mount glusterFS volume
# service apache2 start
#
# Carry out specific functions when asked to by the system
case &amp;quot;$1&amp;quot; in
  start)
    echo &amp;quot;Mounting glusterfs volumes &amp;quot;
    mount -t glusterfs $GLUSTERFS_SERVER:/$GLUSTERFS_VOLUME $MOUNTPOINT
    ;;
  stop)
    echo &amp;quot;Unmount glusterfs volumes&amp;quot;
    umount $MOUNTPOINT
    ;;
  *)
    echo &amp;quot;Usage: /etc/init.d/glusterfs-mount {start|stop}&amp;quot;
    exit 1
    ;;
esac
exit 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Ya solo nos queda dar permisos de ejecución a nuestro script y configurarlo para que se
ejecute durante el inicio del sistema:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod +x /etc/init.d/glusterfs-mount
update-rc.d glusterfs-mount defaults
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;FAQ&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; No puedo crear el anillo de confianza. Los comandos &lt;code&gt;gluster peer probe&lt;/code&gt; terminan
fallando con un timeout ¿Qué ocurre?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Es muy probable que sea un problema de conectividad entre ambos GlusterFS Servers.
Revisa si hay algún firewall entre ellos que pueda estar bloqueando conexiones TCP y UDP
entre ellos. Puedes ver los puertos abiertos con &lt;code&gt;netstat -ntlp&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Si estás en AWS, asegúrate que el &lt;strong&gt;Security Group&lt;/strong&gt; que usan ambas instancias tienen
permitidos los accesos a puerto TCP y UDP entre ellos.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; No puedo montar el volumen compartido desde las instancias que actuan como
clientes. El comando &lt;code&gt;mount&lt;/code&gt; responde con &lt;code&gt;mount failed&lt;/code&gt;. ¿Qué puede estar ocurriendo?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Al igual que con los servidores GlusterFS, revisa que las instancias clientes
tienen conectividad con los servidores. GlusterFS suele abrir un rango de puertos para
cada volumen exportado (gv0, gv1, gv2, etc). Es recomendable permitir todo el tráfico TCP
y UDP entre las instancias clientes y servidor. De otra forma es probable que alguna vez
tengamos problemas al montar los volumenes.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Sigo sin poder crear el punto de montaje desde las instancias clientes. El
comando &lt;code&gt;mount&lt;/code&gt; indica que hubo un problema. La conectividad entre cliente y servidor de
GlusterFS es correcta ¿Qué problema puede haber?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Es muy común que las versiones de las distribuciones entre cliente y servidor
difieran. Si esto es así, es muy probable que la versión de &lt;code&gt;glusterfs-client&lt;/code&gt; difiera de
la versión de &lt;code&gt;glusterfs-server&lt;/code&gt;. En este caso, es muy probable que no podamos montar el
volumen compartido con el driver &lt;code&gt;glusterfs&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GlusterFS permite montar los volumenes con el driver de NFS. En este tipo de situaciones
es normal recurrir al driver de NFS para crear el punto de montaje. Para ello se necesita
tener instalado &lt;code&gt;nfs-common&lt;/code&gt; y crear el punto de montaje con la siguiente instrucción:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mount -t nfs gfs01.example.com:/gv0 /var/www/shared
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; ¿Por qué crear un script en &lt;code&gt;/etc/init.d&lt;/code&gt; pudiendo configurar el montaje en el
fichero &lt;code&gt;/etc/fstab&lt;/code&gt;?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Sí, es posible configurar el punto de montaje en &lt;code&gt;/etc/fstab&lt;/code&gt;, pero a diferencia
del resto de punto de montajes, en este caso se trata de un volumen compartido por la red.
Es muy probable que cuando se monten los volumenes de &lt;code&gt;/etc/fstab&lt;/code&gt;, aún no esté disponible
la red en el sistema, con lo cual el montaje no solo fallará, sino que además es posible
que ralentize el arranque del sistema hasta que el intento de montar el volumen por la red
de un Timeout. Por eso es más fiable hacerlo con un script en &lt;code&gt;init.d&lt;/code&gt;, una vez la red ya
esté disponible.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Creo que esto es todo. ¿Encontráis algún otro problema? ¡Dejadlo en los comentarios!
Saludos.&lt;/p&gt;

&lt;p&gt;:wq&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AWS Summit Barcelona 2015</title>
      <link>http://mviera.io/blog/aws-summit-barcelona-2015/</link>
      <pubDate>Thu, 12 Nov 2015 00:00:00 UTC</pubDate>
      
      <guid>http://mviera.io/blog/aws-summit-barcelona-2015/</guid>
      <description>

&lt;p&gt;No suelo viajar mucho por trabajo pero, desde que trabajo en Crononauta, siempre
subo a Barcelona varias veces al año. Y además de trabajar, esta vez hemos
aprovechado para asistir al &lt;strong&gt;AWS Summit 2015 Barcelona&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;El evento de Amazon Web Services, tuvo lugar durante todo la jornada del Jueves
5 de Noviembre en Fira de Barcelona, Montjuïc. Nunca había estado en Fira
Barcelona, con lo cual me impresionó el edificio, el salon de actos principal y
toda la organización del evento. Especialmente el catering, que me pareció muy
bueno. La calidad de la comida era bastante buena, y en ningún momento faltó de
nada. En todo momento había snacks, zumos, agua, café y bollería.&lt;/p&gt;

&lt;p&gt;En cuanto al evento, el AWS Summit Barcelona fue inaugurado por Guillem Veiga,
Head of AWS en Iberia, con una breve introducción de todo lo que íbamos a poder
ver durante la jornada.&lt;/p&gt;

&lt;h3 id=&#34;toc_0&#34;&gt;AWS re:Invent 2015&lt;/h3&gt;

&lt;p&gt;Luego vino &lt;strong&gt;Jeff Barr&lt;/strong&gt;, el Chief Evangelist de AWS; que muchos lo conocemos
ser quien administra y publica en el blog de AWS. Quién mejor que él, que lleva
13 años trabajando en AWS, para contarnos cómo ha ido evolucionando AWS a lo
largo de estos años. Entre otras cosas nos contaba cómo en 2007 AWS sólo contaba
con tres servicios (EC2, RDS y S3), y actualmente en 2015 cuentan con más de 50
servicios, operando en 11 regiones diferentes. Jeff fue
dando paso a ponencias como la de &lt;strong&gt;Luis Bosque&lt;/strong&gt; de &lt;strong&gt;CartoDB&lt;/strong&gt;, que nos
explicó cómo funciona CartoDB, el volumen de datos que manejan y por qué
decidieron migrar a AWS; o los chicos de &lt;strong&gt;BEEVA&lt;/strong&gt; que nos enseñaron su proyecto
de Big Data con algoritmos de Machine Learning para ofrecer información útil a
sus clientes sobre ventas de artículos por zonas, o incluso ventas de productos
relacionados con su sector.&lt;/p&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;AWS Security&lt;/h3&gt;

&lt;p&gt;A continuación vino &lt;strong&gt;Bill Murray&lt;/strong&gt;, pero no el actor de cine, sino el director
de AWS Security. Lo que más me llamó la atención de su ponencia fue la
presentación de un nuevo producto llamado &lt;strong&gt;Amazon Inspector&lt;/strong&gt; que, de momento,
está en &amp;ldquo;preview&amp;rdquo; y sólo está disponible en la región US West (Oregon).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Amazon Inspector&lt;/strong&gt; te permite analizar el comportamiento de las aplicaciones que
ejecutas en AWS y te ayuda a identificar fallos potenciales de seguridad.
Básicamente te permite instalar un agente en las instancias EC2, para poder
ejecutar una serie de checks que te ayudarán a comprobar la seguridad de tu
plataforma. Bill nos enseñó algunos checks básicos como el login SSH con root.
Una vez se ejecutan estos checks, te permite realizar un análisis general de la
seguridad de la plataforma.&lt;/p&gt;

&lt;p&gt;De momento este servicio está algo limitado, porque
los agentes sólo pueden ser instalados en Amazon Linux AMI 2015.03 (o
posteriores) o Ubuntu Server 14.04 LTS, de momento.&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Todo lo que necesitas saber sobre Autoscaling&lt;/h3&gt;

&lt;p&gt;Seguro que si has trabajado ya sobre AWS, esto ya te suene de algo, o al menos
ese era mi caso. Realmente el tema de Autoscaling es un servicio, sin coste
adicional, que ofrece AWS dentro de EC2; y que permite que tu infraestructura
escale horizontalmente, siempre y cuando esté preparada para ello, claro está.&lt;/p&gt;

&lt;p&gt;Realmente, que una plataforma pueda escalar horizontalmente, no sólo depende de
que configuremos o no el Autoscaling en EC2. Para que una plataforma pueda
escalar horizontalmente debe estar bien diseñada, tanto a nivel de sistemas,
separando los servicios / stack de software de forma inteligente; como a nivel
de código, porque la aplicación también debe estar preparada para escalar.&lt;/p&gt;

&lt;p&gt;Si tenemos bien resuelta esta parte, AWS nos proporciona los componentes
necesarios para escalar nuestra aplicación de forma más o menos sencilla,
configurando los siguientes componentes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Launch Configuration&lt;/strong&gt;: permite definir la &amp;ldquo;template&amp;rdquo; de instancia EC2 que vamos a arrancar, en
caso de que necesitemos auto escalar. Podremos definir qué AMI queremos usar, almacenamiento,
security group, etc.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Auto Scaling Group&lt;/strong&gt;: permite crear un grupo de Autoscaling basado en un Launch Configuration.
Básicamente podemos definir el número mínimo de instancias en el grupo de auto escalado, número
máximo de instancias, zonas de disponibilidad sobre la que vamos a operar, si queremos añadir las
instancias bajo un balanceador o ELB (Elastic Load Balancer), etc.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;CloudWatch Alarms&lt;/strong&gt;: &lt;strong&gt;CloudWatch&lt;/strong&gt; es el servicio de monitorización de AWS. Es un servicio que
no supone coste adicional, siempre que no activemos el &lt;em&gt;Detailed Monitoring&lt;/em&gt;. Nos permitirá
monitorizar nuestro Autoscaling Group, y en base a las alertas que configuremos, nuestra
plataforma será capaz de lanzar nuevas instancias cuando sea necesario. Por ejemplo, podremos
arrancar un par de instancias más en el caso de que el uso de CPU del grupo de Autoscaling supere
el 80% durante 10 minutos, y volver a eliminarlas cuando la CPU del Autoscaling group baje del
40% durante 15 minutos.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Últimamente he trabajado mucho con esto, así que seguramente le dedique un post
donde pueda entrar más en detalle.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Gestionando Containers a Escala&lt;/h3&gt;

&lt;p&gt;En esta Live Demo se presentaba el servicio &lt;strong&gt;EC2 Container Service&lt;/strong&gt;, que
permite desplegar, gestionar y escalar contenedores &lt;strong&gt;Docker&lt;/strong&gt; donde ejecutar
aplicaciones, servicios o simplemente scripts para realizar cálculos.&lt;/p&gt;

&lt;p&gt;En mi caso aún no tengo muchos conocimientos sobre Docker, pero si has trabajado
con él, te facilitará la gestión de clusters de contenedores en varias
instancias EC2. Aunque si esto no te gusta siempre puedes montar Docker desde
cero en una instancia EC2.&lt;/p&gt;

&lt;p&gt;La demo que hizo el ponente era desplegar una página web sencilla dentro de un
cluster Docker. Podéis desplegar esta aplicación de ejemplo desde el Dashboard
de ECS.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ECS&lt;/strong&gt; básicamente te abstrae de todo lo referente a la administración de Docker,
y te permite definir un cluster en unos cuantos pasos sencillos. Durante estos
pasos puedes elegir la memoria asignada a cada contenedor, mapeo de puertos
que se suele hacer en Docker, usar un ELB (balanceador) para el cluster, e
incluso definir la cantidad y tipo de instancias EC2 en las que queremos
alojar el cluster.&lt;/p&gt;

&lt;p&gt;En mi caso me apunto estudiar este servicio, porque la verdad es que los
conocimientos que tengo de él son bastante básicos.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Despliegue Continuo en AWS&lt;/h3&gt;

&lt;p&gt;En esta Live Demo se presentaban tres servicios: &lt;strong&gt;AWS CodeCommit&lt;/strong&gt;, &lt;strong&gt;AWS
CodePipeline&lt;/strong&gt; y &lt;strong&gt;AWS CodeDeploy&lt;/strong&gt;. Estos tres servicios están dentro de la
categoría de &lt;em&gt;Developers Tools&lt;/em&gt; y permiten alojar código fuente, Integración
Continua y deploy de código, respectivamente. A decir verdad me perdí la demo
de los dos primeros, pero pude asistir a la de &lt;strong&gt;AWS CodeDeploy&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CodeDeploy&lt;/strong&gt; permite desplegar código fuente de forma
relativamente sencilla. En muy resumidas cuentas, este servicio se basa en tener
un agente ejecutándose en cada una de las instancias y, una vez configurado un
&lt;em&gt;Deployment Group&lt;/em&gt;, es posible especificar una URL hacía un fichero zip alojado
en un Bucket de S3 o repositorio Github. Mediante un fichero de configuración,
que debe ir dentro del repositorio de código, podremos indicar al agente dónde
desplegar el código, o incluso ejecutar comandos tras realizar el deploy, en un
&lt;em&gt;post-hook&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;CodeDeploy&lt;/strong&gt; permite configurar cuándo un deployment se considera &amp;ldquo;ok&amp;rdquo; o &amp;ldquo;fallido&amp;rdquo;.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;One at a Time&lt;/strong&gt;: se considera un deployment fallido si falla en al menos una de las instancias del &lt;em&gt;Deployment Group&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Half at a Time&lt;/strong&gt;: se considera fallido si falla en la mitad de las instancias del &lt;em&gt;Deployment Group&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;All at Once&lt;/strong&gt;: se considera deployment fallido si falla en todas las instancias del &lt;em&gt;Deployment Group&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quizás una de las cosas que no me convencen de CodeDeploy es que se necesita
tener un agente con un puerto abierto en cada instancia, lo cual puede añadir un
punto de fallo en cuanto a seguridad; y que, si no estoy equivocado, no tiene
opción de rollback. Si algo va mal, la única opción es volver a desplegar una
versión del código que sepamos que funciona bien.&lt;/p&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;Utilizando Spot Instances de Amazon EC2&lt;/h3&gt;

&lt;p&gt;Las spot instances han avanzado mucho desde que fueron introducidas por primera
vez. Normalmente el precio las instancias EC2, al margen del tipo de instancia
que se elija con más o menos recursos; varia en función del &lt;em&gt;modelo de compra&lt;/em&gt;
de la instancia. Puede ser &lt;em&gt;On Demand&lt;/em&gt;, &lt;em&gt;Reserved&lt;/em&gt;, o &lt;em&gt;Spot Instances&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Las instancias &lt;em&gt;On Demand&lt;/em&gt; son las de uso más común. Simplemente cada tipo de
instancia en la modalidad de &lt;em&gt;On Demand&lt;/em&gt; tienen un precio fijo. Si arrancas una
instancia &lt;em&gt;On Demand&lt;/em&gt; sabes que el precio que AWS te cobra es el especificado en
la documentación, y el coste total dependerá del tiempo que esté esa instancia
en modo &lt;em&gt;Running&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Pero también existe la modalidad de &lt;em&gt;Spot Instances&lt;/em&gt;. Para no hacer muy pesada
la explicación, básicamente consiste en fijar el precio que te gustaría pagar
por un tipo de instancia, y en el momento en que el precio de mercado esté por
debajo del precio que hayas establecido, podrás arrancar instancias de ese tipo
a ese precio. Es como jugar a bolsa, cuando las acciones caigan por debajo de un
umbral de dinero, compro acciones.&lt;/p&gt;

&lt;p&gt;La modalidad &lt;em&gt;Spot Instances&lt;/em&gt; es muy llamativa a la hora de aplicarla con un
Autoscaling Group, pues podríamos conseguir ahorros bastante significativos,
arrancando &lt;em&gt;Spot Instances&lt;/em&gt; cuando el precio caiga, y mientras tanto seguir
usando &lt;em&gt;On Demand&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;Amazon Elastic Filesystem&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Amazon Elastic Filesystem&lt;/strong&gt; es uno de los servicios que llevo esperando mucho tiempo. De momento, no está disponible en Ireland pero espero que lo estrenen en Europa de aquí a unos meses.&lt;/p&gt;

&lt;p&gt;Uno de los principales problemas a resolver cuando trabajas en una
infraestructura con escalabilidad horizontal, es la compartición de ficheros
entre instancias. Hasta ahora mucha gente se ha configurado su propio cluster
NFS, GlusterFS, e incluso he llegado a ver soluciones que usan un Bucket S3
junto con &lt;em&gt;s3fs&lt;/em&gt;, lo cual me parece muy arriesgado, porque se ha demostrado que
no funciona muy bien. Precisamente esto es lo que viene a resolver &lt;strong&gt;EFS&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Al igual que con el resto de servicios, podremos abstraernos de la creación del
cluster de ficheros y del storage de éste. En pocos minutos podemos contar con
un punto de montaje con varios Terabytes y con una SLA muy alta. Este servicio
realmente se basa en &lt;em&gt;NSFv4&lt;/em&gt;. Estoy deseando poder probar este servicio en algún
proyecto, hacer pruebas de carga y ver si realmente funciona tan bien como se
espera, o si tendré que seguir usando mi propio cluster de GlusterFS para
compartir ficheros entre instancias.&lt;/p&gt;

&lt;p&gt;Y creo que esto es todo, la verdad es que mola mucho asistir a este tipo de
eventos, porque no todo es resolver tickets, leer documentación, gestionar
alertas y atender clientes. En este tipo de charlas siempre se abre un poco la
mente, sobre todo escuchando las experiencias de los demás, en este caso con
AWS.&lt;/p&gt;

&lt;p&gt;En breve escribiré posts relacionados con Amazon Web Services, y espero poder entrar mucho más en detalle.&lt;/p&gt;

&lt;p&gt;Un saludo!&lt;/p&gt;

&lt;p&gt;:wq! blog/aws-summit-barcelona-2015.md&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>